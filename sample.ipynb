{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd552b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # OpenCVで動画と画像処理\n",
    "import torch  # PyTorchでYOLOv5モデル利用\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e404c14",
   "metadata": {},
   "source": [
    "### 1. YOLOv5モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4e47ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-429-g50313d30 Python-3.11.13 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# ========= 1. YOLOv5モデルのロード =========\n",
    "#   yolov5s.ptは自動でダウンロードされます\n",
    "#   deviceを'cpu'に明示することでCPU専用実行になります\n",
    "model = torch.hub.load(\n",
    "    './yolov5',          # クローンしたYOLOv5リポジトリのパス\n",
    "    'custom',            # カスタムモデル（yolov5s等も可）\n",
    "    path='yolov5s.pt',   # 学習済みモデル（デフォルトでOK）\n",
    "    source='local',      # ローカルファイルとして読み込む\n",
    "    device='cpu'         # CPU強制\n",
    ")\n",
    "\n",
    "# ラベル（人、車…などの名前リスト）\n",
    "class_names = model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94758b9",
   "metadata": {},
   "source": [
    "### 2. 動画ファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6e858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= 2. 動画ファイルの読み込み =========\n",
    "video_path = 'data/nihonbashi-short-mini.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"動画が開けません。ファイルパスを確認してください。\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fb08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9e21f2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d531e0ae",
   "metadata": {},
   "source": [
    "### 3. 推論（物体検出）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdded038",
   "metadata": {},
   "source": [
    "YOLOv5の推定(modelメソッド)の返り値について（GPTの解説）\n",
    "- 型:  yolov5.utils.general.Detections\n",
    "- 座標形式ごとの Tensor を返す\n",
    "    - results.xyxy\n",
    "        [x1, y1, x2, y2, conf, cls] （左上と右下の座標）\n",
    "        \n",
    "        | index  | 内容                                  |\n",
    "        | ------ | ------------------------------------- |\n",
    "        | 0      | `x1` バウンディングボックスの左上の x座標 |\n",
    "        | 1      | `y1` バウンディングボックスの左上の y座標 |\n",
    "        | 2      | `x2` バウンディングボックスの右下の x座標 |\n",
    "        | 3      | `y2` バウンディングボックスの右下の y座標 |\n",
    "        | 4      | `conf` 検出確信度 (confidence score)       |\n",
    "        | 5      | `cls` クラスID（0=person, 1=bicycle, ...） | \n",
    "        <br>\n",
    "    - results.xywh\n",
    "        [x_center, y_center, width, height, conf, cls]\n",
    "   \n",
    "        | index  | 内容                                  |\n",
    "        | ------ | ------------------------------------- |\n",
    "        | 0      | `x_center` バウンディングボックスの中心の x座標 |\n",
    "        | 1      | `y_center` バウンディングボックスの中心の y座標 |\n",
    "        | 2      | `width` バウンディングボックスの幅|\n",
    "        | 3      | `height` バウンディングボックスの高さ |\n",
    "        | 4      | `conf` 検出確信度 (confidence score)       |\n",
    "        | 5      | `cls` クラスID（0=person, 1=bicycle, ...） |\n",
    "\n",
    "    - results.xyxyn / results.xywhn\n",
    "        上記を画像サイズで 0〜1に正規化した値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc2d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognition(frame):\n",
    "    results = model(frame[..., ::-1])  # YOLOv5に画像を渡して検出\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0065671",
   "metadata": {},
   "source": [
    "### 4. 結果をフレームに描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d980ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_info_on_mov(results, frame):\n",
    "    for *box, conf, cls in results.xyxy[0].tolist():\n",
    "        # 座標とラベル\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = class_names[int(cls)]\n",
    "        score = float(conf)\n",
    "        # 検出枠\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        # ラベル\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"{label}: {score:.2f}\",\n",
    "            (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe192b",
   "metadata": {},
   "source": [
    "### 描画ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadbadf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # 動画の最後でループ終了\n",
    "\n",
    "    # ========= 3. 推論（物体検出） =========\n",
    "    # OpenCVはBGR⇒YOLOはRGB\n",
    "    results = recognition(frame)\n",
    "    \n",
    "    # ========= 4. 結果をフレームに描画 =========\n",
    "    # 各物体ごとに検出ボックス＆ラベルを追加\n",
    "    draw_info_on_mov(results, frame)\n",
    "    \n",
    "    # ========= 5. ウィンドウ表示 =========\n",
    "    cv2.imshow(\"YOLOv5 Detection\", frame)\n",
    "    # qキーで途中終了\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ========= 6. 後処理 =========\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dbb53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 入力動画を開く\n",
    "cap = cv2.VideoCapture(\"./data/shinjuku.mp4\")\n",
    "\n",
    "# 動画の情報を取得\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)                       # フレームレート\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))       # 横幅\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))      # 縦幅\n",
    "size = (width, height)\n",
    "\n",
    "# 出力設定（ファイル名, FourCC, FPS, サイズ）\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4形式\n",
    "out = cv2.VideoWriter(\"output.mp4\", fourcc, fps, size)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ここで処理を加えることも可能\n",
    "    # 例: グレースケールに変換（保存はカラー指定なのでBGRに戻す必要あり）\n",
    "    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # frame = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    out.write(frame)  # フレームを書き込み\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e33fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
